# -*- coding: utf-8 -*-
"""nlp_with_data_aug.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1foTAJzAKv4xAP2nlGy1eXl7ZG8OFBzvQ

hellooooooooooooooooo
"""

# ============================================
# 1. Import Required Libraries
# ============================================
import pandas as pd
import re
import numpy as np

# ============================================
# 2. Load the Dataset
# ============================================
df = pd.read_csv("/content/SaudiIrony.csv")
TEXT_COLUMN = "Tweets with Decoded emojis"

print("Initial dataset size:", len(df))


# ============================================
# 3. Normalize the text column (convert to string)
# ============================================
df[TEXT_COLUMN] = df[TEXT_COLUMN].astype(str)


# ============================================
# 4. Smart Removal of Corrupted or Meaningless Rows
#    â†’ Only remove tweets that have NO useful content
# ============================================

def is_bad_row(text):
    """
    Determines whether a row should be removed entirely.
    A row is removed ONLY IF the whole tweet is invalid:
      - pure NaN / None / ERROR
      - tweet is ONLY a URL, ONLY a mention, ONLY a hashtag
      - contains no meaningful Arabic/English characters
    """
    # True NaN values
    if pd.isna(text):
        return True

    t = str(text).strip().lower()

    # Fully invalid entries
    if t in ["nan", "none", "null", "", "#error!", "error", "na"]:
        return True

    # Tweet is ONLY a URL
    if re.fullmatch(r"(http\S+|www\.\S+)", t):
        return True

    # Tweet is ONLY a mention
    if re.fullmatch(r"@\w+", t):
        return True

    # Tweet is ONLY a hashtag
    if re.fullmatch(r"#\w+", t):
        return True

    # If tweet contains real language (Arabic/English), keep it
    if re.search(r"[a-zA-Z\u0600-\u06FF]", t):
        return False

    # Otherwise: no valid content â†’ remove it
    return True


# Apply detection
mask_bad = df[TEXT_COLUMN].apply(is_bad_row)
removed_rows = df[mask_bad]

print("\n Rows that will be REMOVED (invalid entries):")
display(removed_rows[[TEXT_COLUMN]].head(174))
print("Total removed:", mask_bad.sum())

# Remove bad rows
df = df[~mask_bad].copy()


# ============================================
# 5. Replace Textual Emojis â†’ Real Emojis
#    (done BEFORE cleaning to preserve meaning)
# ============================================

def extract_emoji_names(text):
    return re.findall(r':[a-zA-Z0-9_+-]+:', str(text))

# The manual emoji map you provided
manual_map = {
 ":1st_place_medal:": "ğŸ¥‡",
    ":adult:": "ğŸ§‘",
    ":algeria:": "ğŸ‡©ğŸ‡¿",
    ":alien:": "ğŸ‘½",
    ":angel:": "ğŸ˜‡",
    ":angry:": "ğŸ˜ ",
    ":anguished:": "ğŸ˜§",
    ":apple:": "ğŸ",
    ":arrow_down:": "â¬‡ï¸",
    ":arrow_heading_down:": "â¤µï¸",
    ":arrow_left:": "â¬…ï¸",
    ":arrow_upper_left:": "â†–ï¸",
    ":art:": "ğŸ¨",
    ":articulated_lorry:": "ğŸš›",
    ":asterisk:": "*ï¸âƒ£",
    ":astonished:": "ğŸ˜²",
    ":austria:": "ğŸ‡¦ğŸ‡¹",
    ":baby:": "ğŸ‘¶",
    ":baby_bottle:": "ğŸ¼",
    ":bahrain:": "ğŸ‡§ğŸ‡­",
    ":balance_scale:": "âš–ï¸",
    ":balloon:": "ğŸˆ",
    ":bangbang:": "â€¼ï¸",
    ":bar_chart:": "ğŸ“Š",
    ":basketball:": "ğŸ€",
    ":black_flag:": "ğŸ´",
    ":black_heart:": "ğŸ–¤",
    ":black_nib:": "âœ’ï¸",
    ":blossom:": "ğŸŒ¼",
    ":blue_heart:": "ğŸ’™",
    ":blush:": "ğŸ˜Š",
    ":books:": "ğŸ“š",
    ":bouquet:": "ğŸ’",
    ":bow:": "ğŸ™‡",
    ":boy:": "ğŸ‘¦",
    ":brain:": "ğŸ§ ",
    ":bread:": "ğŸ",
    ":bricks:": "ğŸ§±",
    ":briefcase:": "ğŸ’¼",
    ":broken_heart:": "ğŸ’”",
    ":bulb:": "ğŸ’¡",
    ":burrito:": "ğŸŒ¯",
    ":bust_in_silhouette:": "ğŸ‘¤",
    ":butterfly:": "ğŸ¦‹",
    ":call_me_hand:": "ğŸ¤™",
    ":camera:": "ğŸ“·",
    ":candy:": "ğŸ¬",
    ":canned_food:": "ğŸ¥«",
    ":cartwheeling:": "ğŸ¤¸",
    ":cd:": "ğŸ’¿",
    ":chart:": "ğŸ’¹",
    ":cherry_blossom:": "ğŸŒ¸",
    ":child:": "ğŸ§’",
    ":clap:": "ğŸ‘",
    ":clapper:": "ğŸ¬",
    ":cloud:": "â˜ï¸",
    ":clown_face:": "ğŸ¤¡",
    ":coffee:": "â˜•",
    ":coffin:": "âš°ï¸",
    ":cold_face:": "ğŸ¥¶",
    ":cold_sweat:": "ğŸ˜°",
    ":collision:": "ğŸ’¥",
    ":comet:": "â˜„ï¸",
    ":confetti_ball:": "ğŸŠ",
    ":confounded:": "ğŸ˜–",
    ":confused:": "ğŸ˜•",
    ":cookie:": "ğŸª",
    ":corn:": "ğŸŒ½",
    ":cow2:": "ğŸ„",
    ":cowboy_hat_face:": "ğŸ¤ ",
    ":crescent_moon:": "ğŸŒ™",
    ":crossed_fingers:": "ğŸ¤",
    ":crossed_swords:": "âš”ï¸",
    ":crown:": "ğŸ‘‘",
    ":cry:": "ğŸ˜¢",
    ":crying_cat_face:": "ğŸ˜¿",
    ":cupid:": "ğŸ’˜",
    ":cursing_face:": "ğŸ¤¬",
    ":cut_of_meat:": "ğŸ¥©",
    ":cyclone:": "ğŸŒ€",
    ":dancer:": "ğŸ’ƒ",
    ":dancers:": "ğŸ‘¯",
    ":dash:": "ğŸ’¨",
    ":diamonds:": "â™¦ï¸",
    ":disappointed:": "ğŸ˜",
    ":disappointed_relieved:": "ğŸ˜¥",
    ":disguised_face:": "ğŸ¥¸",
    ":dizzy:": "ğŸ’«",
    ":dizzy_face:": "ğŸ˜µ",
    ":dog:": "ğŸ¶",
    ":dog2:": "ğŸ•",
    ":dollar:": "ğŸ’µ",
    ":dove:": "ğŸ•Šï¸",
    ":drooling_face:": "ğŸ¤¤",
    ":duck:": "ğŸ¦†",
    ":eagle:": "ğŸ¦…",
    ":ear_of_rice:": "ğŸŒ¾",
    ":egypt:": "ğŸ‡ªğŸ‡¬",
    ":envelope_with_arrow:": "ğŸ“©",
    ":es:": "ğŸ‡ªğŸ‡¸",
    ":european_union:": "ğŸ‡ªğŸ‡º",
    ":evergreen_tree:": "ğŸŒ²",
    ":exploding_head:": "ğŸ¤¯",
    ":expressionless:": "ğŸ˜‘",
    ":eyes:": "ğŸ‘€",
    ":face_with_head_bandage:": "ğŸ¤•",
    ":face_with_thermometer:": "ğŸ¤’",
    ":facepalm:": "ğŸ¤¦",
    ":fairy:": "ğŸ§š",
    ":fallen_leaf:": "ğŸ‚",
    ":fearful:": "ğŸ˜¨",
    ":female_sign:": "â™€ï¸",
    ":fire:": "ğŸ”¥",
    ":fish:": "ğŸŸ",
    ":fist:": "âœŠ",
    ":fist_right:": "ğŸ¤œ",
    ":flight_departure:": "ğŸ›«",
    ":flushed:": "ğŸ˜³",
    ":foot:": "ğŸ¦¶",
    ":footprints:": "ğŸ‘£",
    ":four_leaf_clover:": "ğŸ€",
    ":fr:": "ğŸ‡«ğŸ‡·",
    ":frog:": "ğŸ¸",
    ":frowning_face:": "â˜¹ï¸",
    ":frowning_person:": "ğŸ™",
    ":fu:": "ğŸ–•",
    ":fuelpump:": "â›½",
    ":full_moon_with_face:": "ğŸŒ",
    ":gem:": "ğŸ’",
    ":ghost:": "ğŸ‘»",
    ":goat:": "ğŸ",
    ":green_heart:": "ğŸ’š",
    ":grimacing:": "ğŸ˜¬",
    ":grin:": "ğŸ˜",
    ":grinning:": "ğŸ˜€",
    ":gun:": "ğŸ”«",
    ":hand_over_mouth:": "ğŸ¤­",
    ":handshake:": "ğŸ¤",
    ":hatching_chick:": "ğŸ£",
    ":hear_no_evil:": "ğŸ™‰",
    ":heart:": "â¤ï¸",
    ":heart_decoration:": "ğŸ’Ÿ",
    ":heart_eyes:": "ğŸ˜",
    ":heart_eyes_cat:": "ğŸ˜»",
    ":heartbeat:": "ğŸ’“",
    ":heartpulse:": "ğŸ’—",
    ":hearts:": "â¤ï¸",
    ":heavy_check_mark:": "âœ”ï¸",
    ":heavy_dollar_sign:": "ğŸ’²",
    ":heavy_exclamation_mark:": "â—",
    ":heavy_heart_exclamation:": "â£ï¸",
    ":herb:": "ğŸŒ¿",
    ":hibiscus:": "ğŸŒº",
    ":high_heel:": "ğŸ‘ ",
    ":hiking_boot:": "ğŸ¥¾",
    ":hot_face:": "ğŸ¥µ",
    ":hourglass_flowing_sand:": "â³",
    ":house:": "ğŸ ",
    ":house_with_garden:": "ğŸ¡",
    ":hugs:": "ğŸ¤—",
    ":hushed:": "ğŸ˜¯",
    ":imp:": "ğŸ‘¿",
    ":inbox_tray:": "ğŸ“¥",
    ":information_desk_person:": "ğŸ’",
    ":innocent:": "ğŸ˜‡",
    ":interrobang:": "â‰ï¸",
    ":iran:": "ğŸ‡®ğŸ‡·",
    ":iraq:": "ğŸ‡®ğŸ‡¶",
    ":it:": "ğŸ‡®ğŸ‡¹",
    ":japanese_goblin:": "ğŸ‘º",
    ":japanese_ogre:": "ğŸ‘¹",
    ":joy:": "ğŸ˜‚",
    ":joy_cat:": "ğŸ˜¹",
    ":kaaba:": "ğŸ•‹",
    ":kiss:": "ğŸ’‹",
    ":kissing:": "ğŸ˜—",
    ":kissing_cat:": "ğŸ˜½",
    ":kissing_closed_eyes:": "ğŸ˜š",
    ":kissing_heart:": "ğŸ˜˜",
    ":kissing_smiling_eyes:": "ğŸ˜™",
    ":knife:": "ğŸ”ª",
    ":kuwait:": "ğŸ‡°ğŸ‡¼",
    ":large_blue_diamond:": "ğŸ”·",
    ":leaves:": "ğŸƒ",
    ":lebanon:": "ğŸ‡±ğŸ‡§",
    ":liberia:": "ğŸ‡±ğŸ‡·",
    ":lion:": "ğŸ¦",
    ":loop:": "â¿",
    ":lotus_position:": "ğŸ§˜",
    ":loud_sound:": "ğŸ”Š",
    ":love_letter:": "ğŸ’Œ",
    ":love_you_gesture:": "ğŸ¤Ÿ",
    ":lying_face:": "ğŸ¤¥",
    ":m:": "â“‚ï¸",
    ":male_sign:": "â™‚ï¸",
    ":man:": "ğŸ‘¨",
    ":man_dancing:": "ğŸ•º",
    ":man_shrugging:": "ğŸ¤·â€â™‚ï¸",
    ":maple_leaf:": "ğŸ",
    ":mask:": "ğŸ˜·",
    ":massage:": "ğŸ’†",
    ":mechanical_arm:": "ğŸ¦¾",
    ":medal_military:": "ğŸ–ï¸",
    ":metal:": "ğŸ¤˜",
    ":money_mouth_face:": "ğŸ¤‘",
    ":money_with_wings:": "ğŸ’¸",
    ":moneybag:": "ğŸ’°",
    ":monkey:": "ğŸ’",
    ":monocle_face:": "ğŸ§",
    ":morocco:": "ğŸ‡²ğŸ‡¦",
    ":mortar_board:": "ğŸ“",
    ":mosque:": "ğŸ•Œ",
    ":moyai:": "ğŸ—¿",
    ":muscle:": "ğŸ’ª",
    ":musical_note:": "ğŸµ",
    ":musical_score:": "ğŸ¼",
    ":nail_care:": "ğŸ’…",
    ":nauseated_face:": "ğŸ¤¢",
    ":negative_squared_cross_mark:": "â",
    ":nerd_face:": "ğŸ¤“",
    ":neutral_face:": "ğŸ˜",
    ":new_moon_with_face:": "ğŸŒš",
    ":no_entry:": "â›”",
    ":no_entry_sign:": "ğŸš«",
    ":no_good:": "ğŸ™…",
    ":no_mouth:": "ğŸ˜¶",
    ":nose:": "ğŸ‘ƒ",
    ":notes:": "ğŸ¶",
    ":ok_hand:": "ğŸ‘Œ",
    ":ok_person:": "ğŸ™†",
    ":open_hands:": "ğŸ‘",
    ":open_mouth:": "ğŸ˜®",
    ":orange_heart:": "ğŸ§¡",
    ":orthodox_cross:": "âœï¸",
    ":otter:": "ğŸ¦¦",
    ":owl:": "ğŸ¦‰",
    ":palm_tree:": "ğŸŒ´",
    ":palms_up_together:": "ğŸ™",
    ":parrot:": "ğŸ¦œ",
    ":partly_sunny:": "â›…",
    ":partying_face:": "ğŸ¥³",
    ":pensive:": "ğŸ˜”",
    ":persevere:": "ğŸ˜£",
    ":person_with_veil:": "ğŸ‘°",
    ":pinched_fingers:": "ğŸ¤Œ",
    ":pinching_hand:": "ğŸ¤",
    ":ping_pong:": "ğŸ“",
    ":place_of_worship:": "ğŸ›",
    ":pleading_face:": "ğŸ¥º",
     ":point_down:": "ğŸ‘‡",
    ":point_left:": "ğŸ‘ˆ",
    ":point_right:": "ğŸ‘‰",
    ":point_up:": "â˜ï¸",
    ":point_up_2:": "ğŸ‘†",
    ":postal_horn:": "ğŸ“¯",
    ":pout:": "ğŸ˜¡",
    ":pouting_cat:": "ğŸ˜¾",
    ":pray:": "ğŸ™",
    ":princess:": "ğŸ‘¸",
    ":punch:": "ğŸ‘Š",
    ":purple_heart:": "ğŸ’œ",
    ":qatar:": "ğŸ‡¶ğŸ‡¦",
    ":rabbit:": "ğŸ°",
    ":rainbow:": "ğŸŒˆ",
    ":raised_back_of_hand:": "ğŸ¤š",
    ":raised_eyebrow:": "ğŸ¤¨",
    ":raised_hand:": "âœ‹",
    ":raised_hands:": "ğŸ™Œ",
    ":raising_hand:": "ğŸ™‹",
    ":raising_hand_woman:": "ğŸ™‹â€â™€ï¸",
    ":ram:": "ğŸ",
    ":red_car:": "ğŸš—",
    ":red_circle:": "ğŸ”´",
    ":relaxed:": "â˜ºï¸",
    ":relieved:": "ğŸ˜Œ",
    ":revolving_hearts:": "ğŸ’",
    ":ribbon:": "ğŸ€",
    ":ring:": "ğŸ’",
    ":rofl:": "ğŸ¤£",
    ":roll_eyes:": "ğŸ™„",
    ":rose:": "ğŸŒ¹",
    ":running:": "ğŸƒ",
    ":running_man:": "ğŸƒâ€â™‚ï¸",
    ":running_woman:": "ğŸƒâ€â™€ï¸",
    ":sandwich:": "ğŸ¥ª",
    ":satisfied:": "ğŸ˜†",
    ":saudi_arabia:": "ğŸ‡¸ğŸ‡¦",
    ":scream:": "ğŸ˜±",
    ":scream_cat:": "ğŸ™€",
    ":see_no_evil:": "ğŸ™ˆ",
    ":seedling:": "ğŸŒ±",
    ":shamrock:": "â˜˜ï¸",
    ":sheep:": "ğŸ‘",
    ":shit:": "ğŸ’©",
    ":shoe:": "ğŸ‘",
    ":shrug:": "ğŸ¤·",
    ":shushing_face:": "ğŸ¤«",
    ":skull:": "ğŸ’€",
    ":sleeping:": "ğŸ˜´",
    ":sleepy:": "ğŸ˜ª",
    ":slightly_frowning_face:": "ğŸ™",
    ":slightly_smiling_face:": "ğŸ™‚",
    ":smile:": "ğŸ˜„",
    ":smiley:": "ğŸ˜ƒ",
    ":smiling_face_with_tear:": "ğŸ¥²",
    ":smiling_face_with_three_hearts:": "ğŸ¥°",
    ":smiling_imp:": "ğŸ˜ˆ",
    ":smirk:": "ğŸ˜",
    ":smirk_cat:": "ğŸ˜¼",
    ":smoking:": "ğŸš¬",
    ":snake:": "ğŸ",
    ":snowman_with_snow:": "â˜ƒï¸",
    ":sob:": "ğŸ˜­",
    ":soccer:": "âš½",
    ":sparkles:": "âœ¨",
    ":sparkling_heart:": "ğŸ’–",
    ":speak_no_evil:": "ğŸ™Š",
    ":standing_person:": "ğŸ§",
    ":star2:": "ğŸŒŸ",
    ":star:": "â­",
    ":star_struck:": "ğŸ¤©",
    ":stop_sign:": "ğŸ›‘",
    ":stuck_out_tongue:": "ğŸ˜›",
    ":stuck_out_tongue_closed_eyes:": "ğŸ˜",
    ":stuck_out_tongue_winking_eye:": "ğŸ˜œ",
    ":sun_with_face:": "ğŸŒ",
    ":sunflower:": "ğŸŒ»",
    ":sunglasses:": "ğŸ˜",
    ":sweat:": "ğŸ˜“",
    ":sweat_drops:": "ğŸ’¦",
    ":sweat_smile:": "ğŸ˜…",
    ":syringe:": "ğŸ’‰",
    ":tada:": "ğŸ‰",
    ":telephone_receiver:": "ğŸ“",
    ":thinking:": "ğŸ¤”",
    ":thought_balloon:": "ğŸ’­",
    ":thumbsdown:": "ğŸ‘",
    ":thumbsup:": "ğŸ‘",
    ":tiger2:": "ğŸ…",
    ":tiger:": "ğŸ¯",
    ":timer_clock:": "â²ï¸",
    ":tired_face:": "ğŸ˜«",
    ":tongue:": "ğŸ‘…",
    ":tooth:": "ğŸ¦·",
    ":triumph:": "ğŸ˜¤",
    ":trophy:": "ğŸ†",
    ":tulip:": "ğŸŒ·",
    ":turtle:": "ğŸ¢",
    ":two_hearts:": "ğŸ’•",
    ":uk:": "ğŸ‡¬ğŸ‡§",
    ":umbrella:": "â˜‚ï¸",
    ":unamused:": "ğŸ˜’",
    ":unicorn:": "ğŸ¦„",
    ":united_arab_emirates:": "ğŸ‡¦ğŸ‡ª",
    ":upside_down_face:": "ğŸ™ƒ",
    ":us:": "ğŸ‡ºğŸ‡¸",
    ":v:": "âœŒï¸",
    ":violin:": "ğŸ»",
    ":vomiting_face:": "ğŸ¤®",
    ":walking:": "ğŸš¶",
    ":walking_man:": "ğŸš¶â€â™‚ï¸",
    ":warning:": "âš ï¸",
    ":wave:": "ğŸ‘‹",
    ":weary:": "ğŸ˜©",
    ":white_check_mark:": "âœ…",
    ":white_heart:": "ğŸ¤",
    ":wilted_flower:": "ğŸ¥€",
    ":wink:": "ğŸ˜‰",
    ":woman:": "ğŸ‘©",
    ":woman_facepalming:": "ğŸ¤¦â€â™€ï¸",
    ":woozy_face:": "ğŸ¥´",
    ":worried:": "ğŸ˜Ÿ",
    ":writing_hand:": "âœï¸",
    ":x:": "âŒ",
    ":yellow_heart:": "ğŸ’›",
    ":yum:": "ğŸ˜‹",
    ":zany_face:": "ğŸ¤ª",
    ":zap:": "âš¡",
    ":zebra:": "ğŸ¦“",
    ":zzz:": "ğŸ’¤"
}
def replace_textual_emojis(text):
    text = str(text)
    for k, v in manual_map.items():
        text = text.replace(k, v)
    return text

df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(replace_textual_emojis)


# ============================================
# 6. Clean tweets WITHOUT deleting useful content
# ============================================
def clean_tweet(text):
    text = str(text)

    # Remove repeated character elongation (e.g., "Ø­Ù„ÙˆÙˆÙˆÙˆ" â†’ "Ø­Ù„ÙˆÙˆ")
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)

    # Remove Arabic diacritics
    text = re.sub(r"[\u0617-\u061A\u064B-\u0652]", "", text)

    # Remove Tatweel
    text = text.replace("Ù€", "")

    # Normalize Alef variants to bare Alef
    text = re.sub(r"[Ø¥Ø£Ø¢Ù±]", "Ø§", text)

    # Normalize Teh Marbuta to Heh
    text = text.replace("Ø©", "Ù‡")

    # Replace ï·º with the full phrase
    text = text.replace("ï·º", "ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…")

    # Remove URLs inside sentences
    text = re.sub(r"http\S+|www\.\S+", " ", text)

    # Remove mentions inside sentences
    text = re.sub(r"@\w+", " ", text)

    # Keep hashtag words but remove the #
    text = text.replace("#", "")

    # Remove unwanted characters but keep Arabic, English, and emojis
    text = re.sub(
        r"[^\w\s\u0600-\u06FF"
        r"\U0001F600-\U0001F64F"
        r"\U0001F300-\U0001F5FF"
        r"\U0001F680-\U0001F6FF"
        r"\U0001F1E6-\U0001F1FF]",
        " ",
        text
    )

    # Remove digits
    text = re.sub(r"\d+", " ", text)

    # Normalize excessive spaces
    text = re.sub(r"\s+", " ", text).strip()

    return text


df["clean_text"] = df[TEXT_COLUMN].apply(clean_tweet)


# ============================================
# 7. Show cleaning results
# ============================================
print("\nDataset size AFTER cleaning:", len(df))

print("\n Sample of cleaned tweets:")
display(df[["clean_text"]].head())


# ============================================
# 8. Save the final cleaned dataset
# ============================================
output_path = "/content/SaudiIrony_clean.csv"
df.to_csv(output_path, index=False)

print("\n Cleaning completed! Final dataset saved as:", output_path)

df = pd.read_csv("/content/SaudiIrony_clean.csv")

df

"""ÙØµÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ø³Ø§Ø®Ø±Ø© ÙˆØºÙŠØ± Ø§Ù„Ø³Ø§Ø®Ø±Ø©"""

df["label"] = df["Final Annotation"].map({
    "ØªÙ‡ÙƒÙ…": 1,
    "Ù„ÙŠØ³Øª ØªÙ‡ÙƒÙ…": 0
})

print(df["label"].value_counts())

LABEL_COL = "label"
TEXT_COL = "clean_text"

df_irony = df[df[LABEL_COL] == 1].copy()
df_non_irony = df[df[LABEL_COL] == 0].copy()

print("Ironic tweets:", len(df_irony))
print("Non-ironic tweets:", len(df_non_irony))

"""Ø§Ø³ØªØ®Ø±Ø§Ø¬ Bi-grams Ùˆ Tri-grams Ù„Ù„ÙØ¦Ø© Ø§Ù„Ø³Ø§Ø®Ø±Ø© ÙÙ‚Ø·"""

from sklearn.feature_extraction.text import CountVectorizer

def get_top_ngrams(corpus, n=2, top_k=40):
    vec = CountVectorizer(ngram_range=(n, n), min_df=3, max_df=0.8)
    bag = vec.fit_transform(corpus)

    sum_words = bag.sum(axis=0)
    words_freq = [(word, sum_words[0, idx])
                  for word, idx in vec.vocabulary_.items()]

    sorted_words = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return sorted_words[:top_k]

bigrams = get_top_ngrams(df_irony[TEXT_COL], n=2, top_k=30)
trigrams = get_top_ngrams(df_irony[TEXT_COL], n=3, top_k=30)

print("Top sarcastic bigrams:", bigrams[:10])
print("Top sarcastic trigrams:", trigrams[:10])

"""Augmentation Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ø§Ø®Ø±Ø©"""

import random

sarcastic_bigrams = [w for w, c in bigrams]
sarcastic_trigrams = [w for w, c in trigrams]

def augment_sarcasm(text):
    tokens = text.split()

    # Ø§Ø®ØªÙŠØ§Ø± n-gram Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ù‹Ø§
    ngram = random.choice(sarcastic_bigrams + sarcastic_trigrams)

    strategies = []

    # 1 â€” Ø¥Ø¶Ø§ÙØ© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    strategies.append(ngram + " " + text)

    # 2 â€” Ø¥Ø¶Ø§ÙØ© ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    strategies.append(text + " " + ngram)

    # 3 â€” Ø¥Ø¯Ø®Ø§Ù„ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ù…Ù„Ø©
    if len(tokens) > 3:
        insert_pos = random.randint(1, len(tokens)-2)
        new_tokens = tokens[:insert_pos] + ngram.split() + tokens[insert_pos:]
        strategies.append(" ".join(new_tokens))

    return random.choice(strategies)

import random

sarcastic_bigrams = [w for w, c in bigrams]
sarcastic_trigrams = [w for w, c in trigrams]

def augment_sarcasm(text):
    tokens = text.split()

    # Ø§Ø®ØªÙŠØ§Ø± n-gram Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ù‹Ø§
    ngram = random.choice(sarcastic_bigrams + sarcastic_trigrams)

    strategies = []

    # 1 â€” Ø¥Ø¶Ø§ÙØ© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    strategies.append(ngram + " " + text)

    # 2 â€” Ø¥Ø¶Ø§ÙØ© ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    strategies.append(text + " " + ngram)

    # 3 â€” Ø¥Ø¯Ø®Ø§Ù„ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ù…Ù„Ø©
    if len(tokens) > 3:
        insert_pos = random.randint(1, len(tokens)-2)
        new_tokens = tokens[:insert_pos] + ngram.split() + tokens[insert_pos:]
        strategies.append(" ".join(new_tokens))

    return random.choice(strategies)

"""ØªØ·Ø¨ÙŠÙ‚ Augmentation ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ø³Ø§Ø®Ø±Ø©"""

df_aug = df_irony.copy()
df_aug[TEXT_COL] = df_aug[TEXT_COL].apply(augment_sarcasm)

"""Ø¯Ù…Ø¬ Ø§Ù„Ø¯Ø§ØªØ§Ø³Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© + augmented"""

df_final = pd.concat([
    df_non_irony,
    df_irony,
    df_aug
], ignore_index=True)

df_final.to_csv("/content/SaudiIrony_augmented.csv", index=False)

print("Augmented dataset saved successfully!")

"""Ø§Ù„Ø§Ù† Ø¨Ù†Ø¯Ø±Ø¨ Ø§Ù„Ù…ÙˆØ¯Ù„"""

!pip install transformers datasets evaluate -q

import pandas as pd
import torch
import torch.nn as nn
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from datasets import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    DataCollatorWithPadding,
    Trainer,
    EarlyStoppingCallback
)

import evaluate

df = pd.read_csv("/content/SaudiIrony_augmented.csv")

df["clean_text"] = df["clean_text"].astype(str)
df["label"] = df["label"].astype(int)

print("Total samples:", len(df))
df.head()

train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df["label"]
)

train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))
test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))

model_name_ara = "aubmindlab/bert-base-arabertv02"
tokenizer_ara = AutoTokenizer.from_pretrained(model_name_ara)

def tokenize(batch):
    return tokenizer_ara(batch["clean_text"], truncation=True)

train_tok = train_ds.map(tokenize, batched=True)
test_tok  = test_ds.map(tokenize, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer_ara)

class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.array([0, 1]),
    y=train_df["label"].values
)

class_weights = torch.tensor(class_weights, dtype=torch.float32).to("cuda")
class_weights

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class_weights = class_weights.to(device)

model_ara = AutoModelForSequenceClassification.from_pretrained(
    model_name_ara,
    num_labels=2
).to(device)

loss_fn = nn.CrossEntropyLoss(weight=class_weights)

class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs["labels"]
        outputs = model(**inputs)
        logits = outputs["logits"]
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(-1)

    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    }

training_args = TrainingArguments(
    output_dir="arabert_old_version",
    num_train_epochs=4,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    logging_steps=50,
    report_to="none"
)

trainer = WeightedTrainer(
    model=model_ara,
    args=training_args,
    train_dataset=train_tok,
    eval_dataset=test_tok,
    tokenizer=tokenizer_ara,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.save_model("arabert_old_finetuned")
tokenizer_ara.save_pretrained("arabert_old_finetuned")

print("\nFinal Evaluation:")
print(trainer.evaluate())

import numpy as np

# Ø¹Ù…Ù„ ØªÙˆÙ‚Ø¹Ø§Øª Ø¹Ù„Ù‰ test_tok
predictions = trainer.predict(test_tok)

# logits â†’ ØªÙ†Ø¨Ø¤Ø§Øª
logits = predictions.predictions
preds = np.argmax(logits, axis=-1)

# labels Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
labels = predictions.label_ids

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(labels, preds)

plt.figure(figsize=(6,4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["ØºÙŠØ± ØªÙ‡ÙƒÙ…", "ØªÙ‡ÙƒÙ…"],
    yticklabels=["ØºÙŠØ± ØªÙ‡ÙƒÙ…", "ØªÙ‡ÙƒÙ…"]
)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import classification_report

report = classification_report(labels, preds, target_names=["ØºÙŠØ± ØªÙ‡ÙƒÙ…", "ØªÙ‡ÙƒÙ…"])
print(report)

import matplotlib.pyplot as plt

logs = trainer.state.log_history

train_loss = []
eval_loss = []
epochs_train = []
epochs_eval = []

for entry in logs:
    if "loss" in entry and "epoch" in entry and "eval_loss" not in entry:
        train_loss.append(entry["loss"])
        epochs_train.append(entry["epoch"])

    if "eval_loss" in entry:
        eval_loss.append(entry["eval_loss"])
        epochs_eval.append(entry["epoch"])

plt.figure(figsize=(8,5))

plt.plot(epochs_train, train_loss, label="Training Loss")
plt.plot(epochs_eval, eval_loss, "o-", label="Validation Loss", markersize=8)

plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

"""Ø§Ù„Ù…ÙˆØ¯Ù„ Ø§Ù„Ø«Ø§Ù†ÙŠ"""

# ============================================
# 1) Loading AraELECTRA Model + Tokenizer
# ============================================

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
import evaluate

model_name_electra = "aubmindlab/araelectra-base-discriminator"

tokenizer_electra = AutoTokenizer.from_pretrained(model_name_electra)

model_electra = AutoModelForSequenceClassification.from_pretrained(
    model_name_electra,
    num_labels=2
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_electra.to(device)

# ============================================
# 2) Tokenization
# ============================================

def tokenize(batch):
    return tokenizer_electra(batch["clean_text"], truncation=True)

train_tok = train_ds.map(tokenize, batched=True)
test_tok  = test_ds.map(tokenize, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer_electra)

# ============================================
# 3) Metrics
# ============================================

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    }

# ============================================
# 4) Training Arguments  (Ù†ÙØ³ Ø§Ù„Ù„ÙŠ Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆÙ†Ù†Ù‚)
# ============================================

training_args = TrainingArguments(
    output_dir="araelectra_old_version",
    num_train_epochs=4,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    logging_steps=50,
    report_to="none"
)

# ============================================
# 5) Trainer
# ============================================

trainer = Trainer(
    model=model_electra,
    args=training_args,
    train_dataset=train_tok,
    eval_dataset=test_tok,
    tokenizer=tokenizer_electra,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# ============================================
# 6) Train + Evaluate
# ============================================

trainer.train()

results = trainer.evaluate()
print("\nAraELECTRA RESULTS:", results)

trainer.save_model("araelectra_finetuned")
tokenizer_electra.save_pretrained("araelectra_finetuned")

"""Ø§Ù„Ø§Ù†Ø³Ù…Ø¨Ù„"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

# ================================
# 1) Load AraBERT
# ================================
arabert_path = "arabert_old_finetuned"
tokenizer_bert = AutoTokenizer.from_pretrained(arabert_path)
model_bert = AutoModelForSequenceClassification.from_pretrained(arabert_path)
model_bert.to("cuda")
model_bert.eval()

# ================================
# 2) Load AraELECTRA
# ================================
electra_path = "araelectra_finetuned"
tokenizer_electra = AutoTokenizer.from_pretrained(electra_path)
model_electra = AutoModelForSequenceClassification.from_pretrained(electra_path)
model_electra.to("cuda")
model_electra.eval()

# ================================
# 3) Softmax probability for ANY text batch
# ================================
def get_probs(model, tokenizer, text_batch):
    inputs = tokenizer(text_batch, return_tensors="pt", truncation=True, padding=True).to("cuda")
    with torch.no_grad():
        logits = model(**inputs).logits
    probs = torch.softmax(logits, dim=-1).cpu().numpy()
    return probs

# ================================
# 4) Ensemble predict for a single input
# ================================
def ensemble_predict(text, w1=0.5, w2=0.5):
    p1 = get_probs(model_bert, tokenizer_bert, [text])
    p2 = get_probs(model_electra, tokenizer_electra, [text])
    final_probs = (w1 * p1) + (w2 * p2)
    final_label = np.argmax(final_probs, axis=1)[0]
    return final_label, final_probs

# ================================
# 5) Evaluation with batching to avoid OOM
# ================================
texts = test_df["clean_text"].tolist()
true_labels = test_df["label"].values

batch_size = 32    # ØªÙ‚Ø¯Ø±ÙŠÙ† ØªØ®ÙØ¶ÙŠÙ†Ù‡ Ù„Ù€ 16 Ø¥Ø°Ø§ ØªØ¨ÙŠÙ†

all_preds = []
all_probs = []

for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]

    # Get model probabilities
    p1 = get_probs(model_bert, tokenizer_bert, batch_texts)
    p2 = get_probs(model_electra, tokenizer_electra, batch_texts)

    # Ensemble averaging
    final_p = (p1 + p2) / 2

    batch_preds = np.argmax(final_p, axis=1)

    all_preds.extend(batch_preds)
    all_probs.append(final_p)

all_probs = np.vstack(all_probs)

# ================================
# Metrics
# ================================
acc = accuracy_score(true_labels, all_preds)
f1 = f1_score(true_labels, all_preds, average="macro")
cm = confusion_matrix(true_labels, all_preds)

print("\n================ ENSEMBLE FINAL RESULTS ================\n")
print(f"Accuracy: {acc:.4f}")
print(f"Macro F1: {f1:.4f}\n")
print("Confusion Matrix:")
print(cm)
print("\nClassification Report:")
print(classification_report(true_labels, all_preds))


# ================================
# 6) single text example (optional)
# ================================
text = "ÙŠØ§ Ø´ÙŠØ® ÙƒÙ„Ø§Ù…Ùƒ Ù…Ø§ ÙŠØ¯Ø®Ù„ Ø§Ù„Ø±Ø§Ø³"
label, probs = ensemble_predict(text)

print("\nExample Prediction:")
print("Final Label:", label)
print("Ensemble Probabilities:", probs)

text = " Ø§Ø·Ù‚Ø·Ù‚ Ø¹Ù„ÙŠÙƒ Ø§ØµÙ„Ø§ ÙŠØ§ Ø´ÙŠØ® ÙƒÙ„Ø§Ù…Ùƒ Ù…Ø§ ÙŠØ¯Ø®Ù„ Ø§Ù„Ø±Ø§Ø³"

label, probs = ensemble_predict(text)

print("Final Label:", label)
print("Ensemble Probabilities:", probs)

import numpy as np
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import torch

# ============================================
# 1) Ø¯Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯
# ============================================

def get_probs(model, tokenizer, dataset):
    model.eval()
    all_probs = []

    for batch in dataset:
        inputs = tokenizer(
            batch["clean_text"],
            truncation=True,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            logits = model(**inputs).logits
            probs = torch.softmax(logits, dim=-1).cpu().numpy()
            all_probs.append(probs)

    return np.vstack(all_probs)

# ============================================
# 2) Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
# ============================================

probs_bert = get_probs(model_ara, tokenizer_ara, test_df.to_dict("records"))
probs_electra = get_probs(model_electra, tokenizer_electra, test_df.to_dict("records"))

# ============================================
# 3) Ø¯Ù…Ø¬ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª (Ensemble)
# ============================================

ensemble_probs = (probs_bert + probs_electra) / 2.0
ensemble_preds = ensemble_probs.argmax(axis=1)

# ============================================
# 4) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù€ Labels Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
# ============================================

true_labels = test_df["label"].values

# ============================================
# 5) Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ Metrics
# ============================================

acc = accuracy_score(true_labels, ensemble_preds)
f1 = f1_score(true_labels, ensemble_preds, average="macro")
cm = confusion_matrix(true_labels, ensemble_preds)

print("====== ENSEMBLE FINAL METRICS ======")
print(f"Accuracy: {acc:.4f}")
print(f"F1 Score: {f1:.4f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n")
print(classification_report(true_labels, ensemble_preds))